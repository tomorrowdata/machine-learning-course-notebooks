{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div hidden=True>\n",
    "    author: Marco Angius\n",
    "    company: TomorrowData srl\n",
    "    mail: marco.angius@tomorrowdata.io\n",
    "    notebook-version: nov20-2.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on 2: Unsupervised Learning\n",
    "### Version 2.1\n",
    "\n",
    "This section is meant for learning the Scikit-Learn APIs end provide a playground for machine learning unsupervised tasks.\n",
    "\n",
    "[Scikit-Learn](https://scikit-learn.org/stable/index.html#) is a library for data mining and data analysis. It  includes models for classification, regression and clustering. It is built on top of NumPy. SciPy and matplotlib. \n",
    "\n",
    "For the purpose of this playground, to get familiar with the Scikit-Learn APis, we would use [Toy Datasets](https://scikit-learn.org/stable/datasets/index.html#toy-datasets) available in the library. \n",
    "\n",
    "Datasets in `sklearn.datasets` return a *Bunch*:\n",
    "> Dictionary-like object, the interesting attributes are: ‘data’, the data to learn, ‘target’, the regression targets, ‘DESCR’, the full description of the dataset, and ‘filename’, the physical location of boston csv dataset (added in version 0.20).\n",
    "\n",
    "In this notebook we also use the `sklearn.datasets.make_blobs` and `sklearn.datasets.make_moons` functions which is of help in generating synthetic data for unsupervised tasks. See the [Generated datasets](https://scikit-learn.org/stable/datasets/index.html#generated-datasets) section for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mglearn library \n",
    "For visualizing the results obtained with our models we are going to employ an existing library made by Andreas C. Muller (author of the book *Introduction to Machine Learning with Python*). The library is available in the [github repository](https://github.com/amueller/mglearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mglearn==0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer, make_blobs, fetch_lfw_people, make_moons\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_STATE = 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset load\n",
    "The **breast cancer** and the **faces** datasets are used in this notebook for applying scaling and PCA transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "breast_ds = load_breast_cancer()\n",
    "faces_ds = fetch_lfw_people(min_faces_per_person=20, resize=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 1**\n",
    "- check the datasets. Use `print(ds.DESCR)`to print information for each of them.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data\n",
    "Here we are going to see how the effect of data scaling has an impact on supervised models before introducing unsupervised models. \n",
    "\n",
    "The `MinMaxScaler`, `StandardScaler`, `RobustScaler` e `Normalizer` are sklearn models and follows the same API convention that we have seen in the supervised section. This means you can use `fit()` and `transform()` for preparing the model and then scale the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 2: Scaling Data**\n",
    "\n",
    "\n",
    "Using the *breast cancer* dataset apply the different scalers and check how data has changed.\n",
    "- MinMaxScaler: scales features in between the provided ranges \n",
    "- RobustScaler: scales the features bases on the quartiles\n",
    "- StandardScaler: scales the features based on the mean and the variance (column-wise)\n",
    "- Normalzer: normalizes each row in order to have unit norm (row-wise)\n",
    "\n",
    "You can use the provided function to plot the data and visually see the differences.\n",
    "\n",
    "**NOTE**: Before applying any transformation select two features from the *breast cancer* in order to visualize them.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X, y = breast_ds.data, breast_ds.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = ... # list of your scaled data with \"MinMaxScaler\", \"RobustScaler\", \"StandardScaler\", \"Normalizer\" \n",
    "scaling_features = breast_ds.feature_names\n",
    "names = [\"MinMaxScaler\", \"RobustScaler\", \"StandardScaler\", \"Normalizer\"]\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20, 25))\n",
    "for i, ax_row in enumerate(axes):\n",
    "    ax_0 = ax_row[0]\n",
    "    ax_1 = ax_row[1]\n",
    "    \n",
    "    ax_0.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    ax_1.scatter(scaled[i][:, 0], scaled[i][:, 1], c=y)\n",
    "    ax_0.set_title(\"Original Data\")\n",
    "    ax_1.set_title(names[i])\n",
    "    for ax in [ax_0, ax_1]:\n",
    "        ax.set_xlabel(scaling_features[0])\n",
    "        ax.set_ylabel(scaling_features[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 3: Observing model performances for different scalers**\n",
    "\n",
    "For convenience, a split into training and test sets is already provided for the *breast cancer* dataset (do not focus on the details for now, this will be part of next hands-ons).\n",
    "\n",
    "- `X_train`: features for the training set \n",
    "- `y_train`: labels for the training set\n",
    "- `X_test`: features for the test set\n",
    "- `y_test`: labels for the test set\n",
    "\n",
    "    \n",
    "The model for this experiment is a Support Vector Machine (SVM) for classification. For details about this model please refer to the [scikit-learn dedicated page](https://scikit-learn.org/stable/modules/svm.html#svm-classification) to SVM. \n",
    "    \n",
    "A function is provided in order to test the model performances on different scaling of the same data. The function takes the training and test sets as tuples. Each tuple should contain features and labels.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Tasks: \n",
    "    \n",
    "1. Fit two or more scalers on the training set;\n",
    "2. Transform both training and test sets;\n",
    "3. Use the `compute_perf_SVC()` function for comparing the scaled data with the base model;\n",
    "4. Print the results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/book.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Theory: Scaling Training and Test**\n",
    "\n",
    "\n",
    "Usually when training machine learning models data is split in two sets: one for training and one for test. This allows to have some data for assessing the model generalization performances. When data scaling is applied, the model used for scaling the data should be fit on the *training* dataset. Then, the same scaling model have to be used in the *test set*. If the test set is not scaled the same way as the training set is, performances may be worst!\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def compute_perf_SVC(training_set, test_set): \n",
    "    Xtr, ytr, Xte, yte = *training_set, *test_set\n",
    "    svc = SVC(C=100, gamma=\"auto\", random_state=11)\n",
    "    svc.fit(Xtr, ytr)\n",
    "    score = svc.score(Xte, yte)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxs  = ...\n",
    "robsuts  = ...\n",
    "stds     = ...\n",
    "norms    = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform train\n",
    "Xtr_mms  = ...\n",
    "Xtr_rs   = ...\n",
    "Xtr_ss   = ...\n",
    "Xtr_ns   = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test\n",
    "Xte_mms  = ...\n",
    "Xte_rs   = ...\n",
    "Xte_ss   = ...\n",
    "Xte_ns   = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/chemistry.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Task 1: Observe Scaled Data**\n",
    "\n",
    "Use `pandas.DataFrame()` to construct dataframes starting from different scaled versions of the *breast cancer* data. In order to access the feature names use the `breast_ds.feature_names` attribute. \n",
    "\n",
    "- Use the `df.describe()` method to compare the data ans see the effect of the different scaling algorithms.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "In this section we are going to use PCA for both visualization and features extraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 4: PCA - Visualization**\n",
    "\n",
    "We can visualize the *breast cancer* features using its first two principal components.\n",
    " \n",
    "- Check the original data dimension;\n",
    "- Use `sklearn.decomposition.PCA(n_components)` for selecting the number of principal components and apply PCA on the *breast cancer* features;\n",
    "- Use the provided function `plot_projected_data(X_transformed)` to plot the dataset projected onto the new subspace (defined by the principal components);\n",
    "- Observe the two principal components: each one presents K coefficients and all together they define a base for a new subspace. Use the `plot_heatmap_coefficients(model)` to analyze the coefficients.\n",
    "\n",
    "    \n",
    "Some questions:\n",
    "1. Does scaling affect the PCA ?\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_projected_data(X_transformed, dataset=breast_ds):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Projected Data\")\n",
    "    mglearn.discrete_scatter(X_transformed[:, 0], X_transformed[:, 1], dataset.target)\n",
    "    plt.legend(dataset.target_names, loc=\"best\")\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    plt.xlabel(\"First principal component\", size=13)\n",
    "    plt.ylabel(\"Second principal component\", size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_heatmap_coefficients(model, dataset=breast_ds):\n",
    "    plt.matshow(model.components_, cmap='viridis')\n",
    "    plt.yticks([0, 1], [\"First component\", \"Second component\"])\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(dataset.feature_names)),\n",
    "    dataset.feature_names, rotation=60, ha='left')\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Principal components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/book.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Theory: Coefficients' Interpretation**\n",
    "\n",
    "Principal components represent directions on the original data and they are a combination of the original features. Observing the coefficients' magnitude for one component, they give a clue about the correlation between features for a particular direction. In the case of the first component the are all of the same sign meaning that if we observe points increasing in the component direction also the original features tends to increase as well. \n",
    "\n",
    "For the second component is different due to we have mixed signs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/chemistry.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Task 2: PCA and Model Performances**\n",
    "\n",
    "    \n",
    "Evaluate model performances with and without PCA as it has been already done in exercise 3. Use again `compute_perf_SVC()` function and the *breast cancer* dataset for this purpose.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 5: PCA - Feature Extraction using the Faces dataset**\n",
    "\n",
    "It is possible to use PCA for feature extraction in order to use the new features for training a supervised model and achieve better results in terms of classification scores.\n",
    "\n",
    "- Check dimensionality for the *faces* dataset;\n",
    "- Use the *faces* dataset and apply PCA in order to extract 100 principal components;\n",
    "- Train and test a KNN classifier on the original features (the `check_knn_performances()` function is provided for the purpose);\n",
    "- Train and test a KNN classifier on the extracted PCA components (the `check_knn_performances()` function is provided for the purpose);\n",
    "- Plot, using the provided `plot_pca_face_components()` function, the extracted principal components;\n",
    "\n",
    "Data have been prepared in `X_faces` and `y_faces`.\n",
    "\n",
    "<br>\n",
    "    \n",
    "Extra:\n",
    "- Play with the `whitening` hyper-parameter of PCA and check the results;\n",
    "- Try with a different number of principal components.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "image_shape = faces_ds.images[0].shape\n",
    "def plot_pca_face_components(model):\n",
    "    fix, axes = plt.subplots(3, 5, figsize=(15, 12),\n",
    "    subplot_kw={'xticks': (), 'yticks': ()})\n",
    "    for i, (component, ax) in enumerate(zip(model.components_, axes.ravel())):\n",
    "        ax.imshow(component.reshape(image_shape), cmap='gray')\n",
    "        ax.set_title(\"{}. component\".format((i + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def check_knn_performances(X_train, X_test, y_train, y_test): \n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(\"Test set accuracy: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X, y = faces_ds.data, faces_ds.target\n",
    "\n",
    "# apply a mask in order to make the data skewed (take up to 50 images of each person)\n",
    "mask = np.zeros(y.shape, dtype=np.bool)\n",
    "for target in np.unique(y):\n",
    "    mask[np.where(y == target)[0][:50]] = 1\n",
    "    \n",
    "# scale to greyscale for numeric stability\n",
    "X_faces, y_faces = X[mask] / 255., y[mask]\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_faces, y_faces, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/book.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Theory: PCA for Face Recognition**\n",
    "\n",
    "It is possible to observe each component extracted by PCA from the *Faces* dataset. Some components seams to be extracting differences between the face and the background while others are encoding the lightning differences amongst different zones of the face.  \n",
    "\n",
    "In addition, given some coefficients $b_1, b_2, ... , b_k$ it is possible to express a test point (an image of a person) as a linear combination of those components.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Finally we look into unsupervised learning models and in particular we are going to see **K-Meams** and **DBSCAN**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 6: K-Means**\n",
    "- generate a random dataset with `make_blob()` (by default it has 2 features and 100 samples)\n",
    "- plot the generated random dataset using the `plot_blob(data)` function\n",
    "- apply kmeans to the random dataset using `KMeans(n_clusters=3)`\n",
    "- plot the results with the provided `plot_clusters(data, model)` function \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_clusters(data, model):\n",
    "    # from sklearn \n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=[10,7])\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Pastel1, alpha=0.7,\n",
    "               aspect='auto', origin='lower')\n",
    "\n",
    "    plt.plot(data[:, 0], data[:, 1], '.', markersize=10)\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = model.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=20, linewidths=3,\n",
    "                color='r', zorder=10)\n",
    "    plt.title('K-means clustering', size=15)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"feature 1\", size=13)\n",
    "    plt.ylabel(\"feature 2\", size=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_blob(data):\n",
    "    f = plt.figure(figsize=[10,7])\n",
    "    ax = f.add_subplot()\n",
    "    ax.scatter(X[:, 0], X[:, 1])\n",
    "    ax.set_title(\"Scatter Plot of random blobs\", fontdict={'fontsize':15})\n",
    "    ax.set_xlabel(\"feature 1\", fontdict={'fontsize':13})\n",
    "    ax.set_ylabel(\"feature 2\", fontdict={'fontsize':13})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X, y  = make_blobs(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/chemistry.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Task 3**:  Try different values of k and plot the results. It is also possible to specify the initialization for the centroids.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When K-Means fails\n",
    "K-means has some drawbacks: \n",
    "- it considers only convex shapes (radius of the cluster's centroids)\n",
    "- it assumes cluster of the same size (diameter)\n",
    "- it does not take into account directions' importance \n",
    "- k as hyperparameter\n",
    "\n",
    "Lets consider two cases when kmeans fails to identify potentially \"meaningful\" clusters. The two dataset are already given for this purpose. Then we will see a different model which can solve these limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 7: K-Means Failure**\n",
    "- use the `X_blob` to fit and plot the result of kmeans on a stretched blob dataset\n",
    "- use the `X_moons` to fit and plot the result of kmeans on the moons dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_blob, y_blob = make_blobs(random_state=110, n_samples=600)\n",
    "rng = np.random.RandomState(1200)\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X_blob = np.dot(X_blob, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.05, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DBSCAN\n",
    "We use a more sophisticated clustering algorithm which creates cluster bases on the data point density. One of the advantages of DBSCAN is the needless of setting a k value. For more details about DBSCAN see the scikit-learn [dedicated page](https://scikit-learn.org/stable/modules/clustering.html#dbscan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 8: DBSCAN**\n",
    "- use DBSCAN to solve the above problems (reuse the already provided data)\n",
    "- try different parameters for DBSCAN: \n",
    "    - `eps`: set this to implicitly control the number of clusters\n",
    "    - `min_sample`: in less dense regions determines if a point a noise one or belonging to a cluster\n",
    "- use the `plot_dbscan_cluster()` to observe the results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_dbscan_clusters(model, data):\n",
    "    core_samples_mask = np.zeros_like(model.labels_, dtype=bool)\n",
    "    core_samples_mask[model.core_sample_indices_] = True\n",
    "    labels = model.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each)\n",
    "              for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    plt.figure(figsize=[10, 7])\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = data[class_member_mask & core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=14)\n",
    "\n",
    "        xy = data[class_member_mask & ~core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=6)\n",
    "\n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_, size=15)\n",
    "    plt.xlabel(\"feature 1\", size=13)\n",
    "    plt.ylabel(\"feature 2\", size=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homeworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine_ds = load_wine()\n",
    "print(wine_ds.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/chemistry.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Task**: Apply scaling and PCA to the **Wine** dataset. \n",
    "\n",
    "<br>\n",
    "Do not pay attention now on the model used for classification, this exercise is meant for understanding an important consequence of scaling the data when adopting PCA. \n",
    "\n",
    "1. Split training and test (this has been done for you);\n",
    "2. Check model performances over the original data(use the provided `check_knn_performances` function as we did in the exercises). This is done for having a baseline reference;\n",
    "3. Transform the data with a `StandardScaler` and save it in a different variable (we need it for comparisons);\n",
    "4. Apply PCA to both non-standardized data and the standardized one;\n",
    "5. Check model performances with both version of the data (PCA and Standardization + PCA);\n",
    "6. Plot the first and second component of the data with PCA and the first and second component  of the data with Standardization + PCA (use the provided `plot_scaling_pcs` function);\n",
    "7. Compare the results and observe the difference between the two plots.\n",
    "\n",
    "<hr>\n",
    "    \n",
    "Check the [reference](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py) here for more details (do this after solving the exercise)\n",
    "    \n",
    "[**SOLUTION**](./solutions/handson2/solution.py)\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_scaling_pcs(X_pca, X_scaled_pca, y):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "\n",
    "\n",
    "    for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "        ax1.scatter(X_pca[y == l, 0],\n",
    "                    X_pca[y == l, 1],\n",
    "                    color=c,\n",
    "                    label='class %s' % l,\n",
    "                    alpha=0.5,\n",
    "                    marker=m\n",
    "                    )\n",
    "\n",
    "    for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "        ax2.scatter(X_scaled_pca[y == l, 0],\n",
    "                    X_scaled_pca[y == l, 1],\n",
    "                    color=c,\n",
    "                    label='class %s' % l,\n",
    "                    alpha=0.5,\n",
    "                    marker=m\n",
    "                    )\n",
    "\n",
    "    ax1.set_title('Training dataset after PCA')\n",
    "    ax2.set_title('Standardized training dataset after PCA')\n",
    "\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_xlabel('1st principal component')\n",
    "        ax.set_ylabel('2nd principal component')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/lightbulb.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp; **PCA & Scaling Tip 1**: \n",
    "Be careful to fit both `StandardScaler` and `PCA` on the Training Dataset and use the fit models for transforming the *Test Data*. Why this? Our *Test Data*  simulates new data as if the model is put into production and it should be data you haven't observed yet, thus your only prior information is your training data set.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def check_knn_performances(X_train, X_test, y_train, y_test): \n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(\"Test set accuracy: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# 1: split test and training\n",
    "X, y = wine_ds.data, wine_ds.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=R_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/book.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Theory: Feature Scaling** \n",
    "\n",
    "For some ML applications and models, the data scaling may have an impact on the obtained results and performance provided by a given model. An example is PCA, for which the components that maximize the variance are the key principle of this technique. If there are components which vary more than others, due to their nature, this may affect how principal components are computed.\n",
    "\n",
    "Another application where scaling the feature is a must is for Neural Networks. Why this is important will be a subject of Lecture 4.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div hidden=True>\n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp; Icon made by <a href=\"https://www.flaticon.com/authors/smashicons\" title=\"Smashicons\">Smashicons</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "\n",
    "<img src=\"../resources/icons/lightbulb.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;Icon made by <a href=\"https://www.flaticon.com/authors/pixelmeetup\" title=\"Pixelmeetup\">Pixelmeetup</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "\n",
    "<img src=\"../resources/icons/new.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp; Icon made by <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "\n",
    "<img src=\"../resources/icons/chemistry.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp; Icon made by <a href=\"https://www.flaticon.com/authors/popcorns-arts\" title=\"Icon Pond\">Icon Pond</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "\n",
    "<img src=\"../resources/icons/book.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp; Icon made by <a href=\"https://www.flaticon.com/authors/popcorns-arts\" title=\"Icon Pond\">Icon Pond</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
