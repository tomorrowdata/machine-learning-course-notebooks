{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div hidden=True>\n",
    "    author: Marco Angius\n",
    "    company: TomorrowData srl\n",
    "    mail: marco.angius@tomorrowdata.io\n",
    "    notebook-version: nov20-v2.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on 4: Model Selection\n",
    "### Version 2.1\n",
    "This section aims to give some hints on how evaluating models and selecting parameters. We are going to use supervised models for this scope. We still employ Scikit-learn which offers also a good reference about [this topic](https://scikit-learn.org/stable/model_selection.html#model-selection). \n",
    "\n",
    "The agenda: \n",
    "- Cross Validation \n",
    "- Grid Search\n",
    "- Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mglearn library \n",
    "For visualizing the results obtained with our models we are going to employ an existing library made by Andreas C. Muller (author of the book *Introduction to Machine Learning with Python*). The library is available in the [github repository](https://github.com/amueller/mglearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mglearn==0.1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "In the previous notebooks we have already used `train_test_split` for keeping part of the data outside the model training in order to observe its generalization capabilities on unseen data. Cross Validation allows for more control over the data granting more stability and allows to use all the available data for the training.\n",
    "\n",
    "It does it by splitting the dataset into *folds* and then using the k-1 folds for training and the left fold for testing, repeating the process until all folds are used as test set. This means that the model is trained k times, then the obtained scores on the single folds and the mean score is given.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 1: Cross-Validation**\n",
    "- load the *iris* dataset \n",
    "- select a model for classification (suggestion: `sklearn.linear_model.LogisticRegression`)\n",
    "- use `scores = cross_val_score(model, X, y, cv=numb_folds)` and check the result \n",
    "- try different *numb_folds* values\n",
    "\n",
    "**Disclaimer**: we are using the test set as validation set just for keep things simple. In the next chapter we will see how to prepare the data properly between training, validation and test.\n",
    "    \n",
    "Why some folds performs perfectly and others are performing worse?\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def print_pretty_scores(scores):\n",
    "    print(\"Mean sore on test set: {:.3f}\".format(scores.mean()))\n",
    "    print(\"Std score on test set: {:3f}\".format(scores.std()))\n",
    "    for i, score in enumerate(scores):\n",
    "        print(\"Fold {} - score: {:.3f}\".format(i, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 2: Stratified CV**\n",
    "For classification we might want to preserve the distribution of classes across training and testing. Standard *cross_validation* does not allows this. We can use the `sklearn.model_selection.StratifiedKFold` for taking into account class distributions.\n",
    "\n",
    "- use the iris dataset\n",
    "- instantiate `skf = StratifiedKFold(n_splits)` \n",
    "- use it over the iris dataset by passing the strategy to the `cross_val_score(cv=skf)` \n",
    "- compare the results using `kf = KFold(n_splits, shuffle=False)`\n",
    "\n",
    "What if we do not perform shuffle?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.concatenate((y.reshape(-1,1), np.ones((len(y), 1))), axis=1), \n",
    "             columns=[\"classes\", \"count\"]).groupby(\"classes\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/book.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Theory: Large datasets**\n",
    "\n",
    "In cases when datasets are large another useful cross-validation tool is *shuffle-split cv* which uses a different strategy for creating the splits. It creates n splits and allows for selecting the portion of training points and test points which do not need to sum to one. This is like subsampling the initial dataset and allows for controlling the number of splits without affecting the distribution of samples in the training and test sets. \n",
    "\n",
    "In scikit-learn `ShuffleSplit` and `StratifiedShuffleSplit` for classification are available for this purpose.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "Since now we have used supervised models which number of hyperparameters is limited. There are cases where a model can be tuned with many parameters. We are going to see one of this case with Support Vector Machines and the use of RBF (Radial Basis Function) as a kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 3: Implementation of Grid Search**\n",
    "We keep using the iris dataset (balanced version) for this tasks. \n",
    "\n",
    "- split the iris dataset in training and test using `train_test_split()`\n",
    "- using the provided list of hyper-parameter `gamma` and `C: \n",
    "    - train the `sklearn.svm.SVC()` model using all combinations of the two \n",
    "    - save the best hyper-parameters and the best test score \n",
    "- print the best obtained results\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "gamma = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_model = (None, None)\n",
    "\n",
    "for g in gamma:\n",
    "    for c in C:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with CV\n",
    "What we did before applying Grid Search was to look for the hyper-parameters of our model(s) on the test data.\n",
    "\n",
    "**Using the test data for searching the best hyper-parameter set has to be avoided!**\n",
    "\n",
    "Using the test set leads to being overconfident on selecting the best model (or hyper-parameter set). \n",
    "Indeed, we do not have any information on how the model would behave with new data and due to we used the test for finding the best parameters we cannot use it for evaluating how good is the model. \n",
    "\n",
    "We should split the data further, keeping the test for evaluating the goodness of the model while using another split to select the best combination of hyper-parameters. \n",
    "\n",
    "The new portion we are seeking has to be look on the training data! This split is called **validation set**. We can use cross-validation applied to the training data for deriving different splits of training and validation sets. The best model, the one with best abtained mean score on the different splits is selected. \n",
    "\n",
    "Once selected, all the data from the original training set is used to train the model on the found hyper-parameters. The test set is then used to assess the **generalization** performance of the model to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 4: GridSearchCV**\n",
    "\n",
    "Scikit-learn implement a model which performs both gridsearch and CV. Use the already defined dictionary which encodes the parameter grid and: \n",
    "- instantiate `gscv = GridSearchCV(model, param_grid, cv)` class passing a `SVC()` model and the parameters\n",
    "- split the iris dataset in train and test sets\n",
    "- fit the model on the training set \n",
    "- analyze the obtained scores with the `show_scores_gscv(gscv)` provided function\n",
    "- evaluate the best model:\n",
    "    - get the model `gscv.best_estimator`\n",
    "    - evaluate score for test set\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def show_scores_gscv(model):\n",
    "    res = pd.DataFrame(model.cv_results_)\n",
    "    return (\n",
    "        res[res.columns[4:]].drop(columns=\"params\")\n",
    "        .set_index([\"param_gamma\", \"param_C\"])\n",
    "        .sort_values(\"rank_test_score\")[:5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_ds.data, iris_ds.target, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "gscv = GridSearchCV(SVC(), param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics: unbalancing in classification \n",
    "\n",
    "Let's consider the binary case: it may happen that the problem under study presents much more samples of a class (majority class) with respect to the other class (minority class). An example is when predicting a diagnosis for tumors for which we have the majority of cases to be negative and few samples to be positives. \n",
    "\n",
    "In such cases the dataset is said to be **unbalanced**. \n",
    "\n",
    "The presence of an unbalanced distribution of the classes in the dataset may affect the model training and results. We had a look previously while performing CV in exercise 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 5: Unbalanced Cases**\n",
    "Using the provided dataset (the *digits* dataset casted to be binary) check the performances using the `sklearn.metrics.accuracy_score` with:\n",
    "- A dummy classifier `DummyClassifier`\n",
    "- A decision tree classifier `DecisionTreeClassifier`\n",
    "- A logistic regression `LogisticRegression`\n",
    "\n",
    "\n",
    "Why the *Dummy* model is getting an high score ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "y = y == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix, precision, recall and f-score\n",
    "One way to evaluate unbalanced classification problems is to use a confusion matrix. This allows to check how many points are miss-classified, either a positive class classified as negative or a negative class classified as positive, or correctly classified. The terminology for the confusion matrix is: *True Negative* (TN), *True Positive* (TP), *False Negative* (FN) and *False Positives* (FP).\n",
    "\n",
    "From these it is possible to define the following metrics: \n",
    "- **Precision**: $ \\frac{TP}{TP+FP} $ optimizes the least number of FP miss classified\n",
    "- **Recall**: $ \\frac{TP}{TP+FN} $ optimizes the positive class correctly classified (minority one usually)\n",
    "- **F-score**: $2 * \\frac{precision * recall}{precision + recall} $ harmonic mean of precision and recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;  **Exercise 6: Confusion Matrix**\n",
    "\n",
    "- plot the confusion matrix for the three models used before using the `plot_confusion_matric(y_true, y_pred)` provided function\n",
    "- use `recall_score`, `precision_score`, `f1_score` to print the obtained scores\n",
    "\n",
    "Do you have a better idea of which model is performing better ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes=[\"three\", \"not_three\"],\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "#     classes = classes[unique_labels(np.int0(y_true), np.int0(y_pred))]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div hidden=True>\n",
    "<img src=\"../resources/icons/list.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp; Icon made by <a href=\"https://www.flaticon.com/authors/smashicons\" title=\"Smashicons\">Smashicons</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "\n",
    "<img src=\"../resources/icons/lightbulb.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp;Icon made by <a href=\"https://www.flaticon.com/authors/pixelmeetup\" title=\"Pixelmeetup\">Pixelmeetup</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "\n",
    "<img src=\"../resources/icons/new.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp; Icon made by <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "\n",
    "<img src=\"../resources/icons/chemistry.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp; Icon made by <a href=\"https://www.flaticon.com/authors/popcorns-arts\" title=\"Icon Pond\">Icon Pond</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "\n",
    "<img src=\"../resources/icons/book.png\"  width=\"20\" height=\"20\" align=\"left\"> &nbsp; Icon made by <a href=\"https://www.flaticon.com/authors/popcorns-arts\" title=\"Icon Pond\">Icon Pond</a> from <a href=\"https://www.flaticon.com/\"             title=\"Flaticon\">www.flaticon.com</a>\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
